{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please use the space below to provide your comments about what you believe is our company's greatest opportunity. Your comments will appear exactly as written, and will not be edited in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question = 'OC0002'\n",
    "\n",
    "comment = pd.read_csv('..\\data\\ManuLife_OC0002.csv',index_col = 0,names = ['EmployeeID','Comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comment = comment.fillna('No Comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      Comment\n",
      "EmployeeID                                                   \n",
      "580607                                         Cultural shift\n",
      "580609      I can only think of 2 opportunities at the mom...\n",
      "580611      Most people will leave the company because oth...\n",
      "580612               to be able to get a larger target market\n",
      "580615                Continue to make employees feel valued.\n",
      "(9033, 1)\n"
     ]
    }
   ],
   "source": [
    "print(comment.head())\n",
    "print(comment.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three steps\n",
    "## 1 - tokenize\n",
    "## 2 - remove stop words\n",
    "## 3 - stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make everything lower case\n",
    "comment['Comment'] = comment['Comment'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenize main comment\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "comment['Comment_Tokenized'] = None\n",
    "\n",
    "for i in range(0,comment.shape[0]):\n",
    "    tkn = tokenizer.tokenize(comment.iloc[i,0])\n",
    "    comment['Comment_Tokenized'].iloc[i] = tkn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "                       \n",
    "#remove punctuation from stop words\n",
    "stop = [''.join(c for c in s if c not in string.punctuation) for s in stop]\n",
    "\n",
    "#add in key words from the question\n",
    "question_kw = ['company'\n",
    "               ,'greatest','opportunity','believe']\n",
    "stop.extend(question_kw)\n",
    "\n",
    "comment['Comment_NoStop'] = None\n",
    "\n",
    "for i in range(0,comment.shape[0]):\n",
    "    text = comment['Comment_Tokenized'].iloc[i]\n",
    "    text = [word for word in text if word not in stop]\n",
    "    comment['Comment_NoStop'].iloc[i] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stem the words\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "comment['Comment_stemmed'] = None\n",
    "\n",
    "for i in range(0,comment.shape[0]):\n",
    "    t = comment['Comment_NoStop'].iloc[i]\n",
    "    \n",
    "    l = []\n",
    "\n",
    "    for word in t:\n",
    "        a = stemmer.stem(word)\n",
    "        l.append(a)\n",
    "        \n",
    "    comment['Comment_stemmed'].iloc[i] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_cols = ['EmployeeID','Bigram']\n",
    "bigram_df = pd.DataFrame(columns = bigram_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1 minutes to run\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in range(0,comment.shape[0]):\n",
    "    test = list(nltk.bigrams(comment['Comment_stemmed'].iloc[i]))\n",
    "    eid = comment.index.values[i]\n",
    "    for t in test:\n",
    "        d_dict ={'EmployeeID' : [eid], 'Bigram' : [t]}\n",
    "        df_temp = pd.DataFrame(data = d_dict)\n",
    "        bigram_df = bigram_df.append(df_temp)\n",
    "        \n",
    "t1 = time.time()\n",
    "print('{:.2} minutes to run'.format((t1-t0)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_df = bigram_df.reset_index(drop = True).copy()\n",
    "bigram_df['Bigram1'],bigram_df['Bigram2'] = zip(*bigram_df['Bigram'])\n",
    "bigram_df['Bigram_Clean'] = bigram_df['Bigram1'] + ' ' + bigram_df['Bigram2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comment['Sentiment'] = None\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for i in range(0,comment.shape[0]):\n",
    "    t = comment['Comment'].iloc[i]\n",
    "    ss = sid.polarity_scores(t)\n",
    "    comment['Sentiment'].iloc[i] = ss['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comment.to_csv('~/Desktop/comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_df.to_csv('~/Desktop/bigrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#how do we do topic extraction"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
